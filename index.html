<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

    <title>Yi Qin</title>

    <meta name="author" content="Yi Qin" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link
      rel="shortcut icon"
      href="images/favicon/favicon.ico"
      type="image/x-icon"
    />
    <link rel="stylesheet" type="text/css" href="stylesheet.css" />
  </head>

  <body>
    <table
      style="
        width: 100%;
        max-width: 800px;
        border: 0px;
        border-spacing: 0px;
        border-collapse: separate;
        margin-right: auto;
        margin-left: auto;
      "
    >
      <tbody>
        <tr style="padding: 0px">
          <td style="padding: 0px">
            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr style="padding: 0px">
                  <td style="padding: 2.5%; width: 63%; vertical-align: middle">
                    <p class="name" style="text-align: center">
                      Yi Qin (Eason)
                    </p>
                    <p>
                      I'm now currently a Ph.D. candidate supervised by
                      <a href="https://xmengli.github.io/">Prof. Xiaomeng Li</a>
                      at ECE, the Hong Kong University of Science and
                      Technology. I also had the opportunity to work with
                      <a href="http://www.wanghao.in/">Prof. Hao Wang</a> and
                      <a href="https://people.csail.mit.edu/lumi/"
                        >Prof. Lu Mi</a
                      >. I am also collaborating with Guangdong Cardiovascular
                      Institute and Prince of Wales Hospital on Echocardiography
                      AI. Before joining HKUST, I obtained BEng at the South
                      China University of Technology majoring in Automation
                      Science and Engineering.
                    </p>
                    <p style="text-align: center">
                      <a href="mailto:yqinar@connect.ust.hk">Email</a>
                      &nbsp;/&nbsp; <a href="">CV</a> &nbsp;/&nbsp;
                      <a
                        href="https://scholar.google.com/citations?user=oIcu4mgAAAAJ&hl=zh-CN"
                        >Scholar</a
                      >
                      &nbsp;/&nbsp;
                      <a href="https://twitter.com/YiQin_Eason">Twitter</a>
                      &nbsp;/&nbsp;
                      <a href="https://www.linkedin.com/in/yi-qin-91b531238/"
                        >Linkedin</a
                      >
                    </p>
                  </td>
                  <td style="padding: 2.5%; width: 40%; max-width: 40%">
                    <a href="images/YiQin.png"
                      ><img
                        style="
                          width: 100%;
                          max-width: 100%;
                          object-fit: cover;
                          border-radius: 50%;
                        "
                        alt="profile photo"
                        src="images/YiQin.png"
                        class="hoverZoomLink"
                    /></a>
                  </td>
                </tr>
              </tbody>
            </table>
            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td
                    style="padding: 20px; width: 100%; vertical-align: middle"
                  >
                    <h2>Research</h2>
                    <p>
                      My research interests lie at the intersection of machine
                      intelligence and digital healthcare, with a particular
                      interest on echocardiography and cardiology. Specific
                      interested topics include:
                    </p>
                    <ul>
                      <li>Diffusion-based Generative Model</li>
                      <li>
                        Trustworthy/Explainable ML (Energy-based Concept-based
                        models)
                      </li>
                      <li>
                        Foundation Model (FM) based Disease Diagnosis/Prognosis
                        Prediction
                      </li>
                    </ul>
                  </td>
                </tr>
              </tbody>
            </table>

            <p>
              "*" indicates equal contribution, "_" indicates equal advising.
            </p>

            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td style="padding: 20px; width: 25%; vertical-align: middle">
                    <img
                      src="./images/ECBM.png"
                      alt="b3do"
                      width="160"
                      style="border-style: none"
                    />
                  </td>
                  <td width="75%" valign="middle">
                    <a href="https://openreview.net/forum?id=I1quoTXZzc">
                      <span class="papertitle"
                        >Energy-Based Concept Bottleneck Models: Unifying
                        Prediction, Concept Intervention, and Conditional
                        Interpretations</span
                      >
                    </a>
                    <br />
                    Xinyue Xu, <strong>Yi Qin</strong>, Lu Mi, <u>Hao Wang</u>,
                    <u>Xiaomeng Li</u>
                    <br />
                    <em>ICLR</em>, 2024
                    <br />
                    <a href="https://github.com/xmed-lab/ECBM">Codes</a>
                    <p>
                      We introduce Energy-Based Concept Bottleneck Models (ECBM)
                      as a unified framework for concept-based prediction,
                      concept correction, and fine-grained interpretations based
                      on conditional probabilities.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td style="padding: 20px; width: 25%; vertical-align: middle">
                    <img
                      src="./projects/assets/fsdiffreg/mainfigure.png"
                      alt="b3do"
                      width="160"
                      style="border-style: none"
                    />
                  </td>
                  <td width="75%" valign="middle">
                    <a
                      href="https://link.springer.com/chapter/10.1007/978-3-031-43999-5_62"
                    >
                      <span class="papertitle">
                        FSDiffReg: Feature-wise and Score-wise Diffusion-guided
                        Unsupervised Deformable Image Registration for Cardiac
                        Images</span
                      >
                    </a>
                    <br />
                    <strong>Yi Qin</strong>, <u>Xiaomeng Li</u>
                    <br />
                    <em>MICCAI</em>, 2023
                    <br />
                    <a href="https://easonqin.top/projects/FSDiffReg.html"
                      >Project Page</a
                    >
                    <p>
                      To fully exploit the diffusion model's ability to guide
                      the registration task, we present two modules in
                      FSDiffReg: Feature-wise Diffusion-Guided Module (FDG) and
                      Score-wise Diffusion-Guided Module (SDG).
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td style="padding: 20px; width: 25%; vertical-align: middle">
                    <img
                      src="./images/ECED.png"
                      alt="b3do"
                      width="160"
                      style="border-style: none"
                    />
                  </td>
                  <td width="75%" valign="middle">
                    <a
                      href="https://papers.miccai.org/miccai-2025/paper/3614_paper.pdf"
                    >
                      <span class="papertitle"
                        >Multi-Agent Collaboration for Integrating
                        Echocardiography Expertise in Multi-Modal Large Language
                        Models</span
                      >
                    </a>
                    <br />
                    <strong>Yi Qin</strong>, Dinusara Sasindu Gamage
                    Nanayakkara, <u>Xiaomeng Li</u>
                    <br />
                    <em>MICCAI</em>, 2025
                    <br />
                    <a
                      href="https://papers.miccai.org/miccai-2025/paper/3614_paper.pdf"
                      >Paper</a
                    >
                    <p>
                      We propose Multi-Agent Collaborative Expertise Extractor,
                      a multi-agent system that builds EchoCardiography
                      Expertise Database, the richest cardiac knowledge base
                      from diverse sources. We also introduce Echocardiography
                      Expertise-enhanced Visual Instruction Tuning, a
                      lightweight tuning method that efficiently injects this
                      expertise into models by training less than 1% of
                      parameters.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- NEW -->
            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td style="padding: 20px; width: 25%; vertical-align: middle">
                    <img
                      src="./images/EVCLIP.png"
                      alt="b3do"
                      width="160"
                      style="border-style: none"
                    />
                  </td>
                  <td width="75%" valign="middle">
                    <a
                      href="https://papers.miccai.org/miccai-2025/paper/4443_paper.pdf"
                    >
                      <span class="papertitle"
                        >EchoViewCLIP: Advancing Video Quality Control through
                        High-performance View Recognition of
                        Echocardiography</span
                      >
                    </a>
                    <br />
                    Shanshan Song, <strong>Yi Qin</strong>, Honglong Yang,
                    Taoran Huang, Hongwen Fei, <u>Xiaomeng Li</u>
                    <br />
                    <em>MICCAI</em>, 2025
                    <br />
                    <a
                      href="https://papers.miccai.org/miccai-2025/paper/4443_paper.pdf"
                      >Paper</a
                    >
                    <p>
                      EchoViewCLIP addresses these issues using a large dataset
                      with 38 standard views and OOD samples. It introduces a
                      Temporal-informed Multi-Instance Learning (TML) module for
                      capturing key frames and a Negation Semantic-Enhanced
                      (NSE) detector for OOD rejection. A quality assessment
                      branch boosts reliability. The model achieves 96.1%
                      accuracy, advancing fine-grained view recognition and
                      robust OOD handling in echocardiography.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td style="padding: 20px; width: 25%; vertical-align: middle">
                    <img
                      src="./images/CVGDiff.png"
                      alt="b3do"
                      width="160"
                      style="border-style: none"
                    />
                  </td>
                  <td width="75%" valign="middle">
                    <a
                      href="https://papers.miccai.org/miccai-2025/paper/0033_paper.pdf"
                    >
                      <span class="papertitle"
                        >Cross-view Generalized Diffusion Model for Sparse-view
                        CT Reconstruction</span
                      >
                    </a>
                    <br />
                    Jixiang Chen, Yiqun Lin, <strong>Yi Qin</strong>, Hualiang
                    Wang, <u>Xiaomeng Li</u>
                    <br />
                    <em>MICCAI</em>, 2025
                    <br />
                    <a
                      href="https://papers.miccai.org/miccai-2025/paper/0033_paper.pdf"
                      >Code</a
                    >
                    <p>
                      We introduce CvG-Diff, a fast, high-quality sparse-view CT
                      reconstruction method that models artifacts
                      deterministically and introduces two novel techniques—EPCT
                      and SPDPS—to reduce errors and improve efficiency.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td style="padding: 20px; width: 25%; vertical-align: middle">
                    <img
                      src="./images/XMedGPT.png"
                      alt="b3do"
                      width="160"
                      style="border-style: none"
                    />
                  </td>
                  <td width="75%" valign="middle">
                    <a href="https://arxiv.org/abs/2505.06898">
                      <span class="papertitle"
                        >Multi-Modal Explainable Medical AI Assistant for
                        Trustworthy Human-AI Collaboration</span
                      >
                    </a>
                    <br />
                    Honglong Yang, Shanshan Song, <strong>Yi Qin</strong>, Lehan
                    Wang, Haonan Wang, Xinpeng Ding, Qixiang Zhang, Bodong Du,
                    <u>Xiaomeng Li</u>
                    <br />
                    Arxiv
                    <br />
                    <a href="https://arxiv.org/abs/2505.06898">Paper</a>
                    <p>
                      We introduce XMedGPT, a multi-modal medical AI assistant
                      that enhances clinical usability by combining accurate
                      diagnostics with visual-text explainability and
                      uncertainty quantification, enabling transparent and
                      trustworthy decision-making.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td style="padding: 20px; width: 25%; vertical-align: middle">
                    <img
                      src="./images/RCMed.png"
                      alt="b3do"
                      width="160"
                      style="border-style: none"
                    />
                  </td>
                  <td width="75%" valign="middle">
                    <a href="https://arxiv.org/abs/2505.03380">
                      <span class="papertitle"
                        >Reinforced Correlation Between Vision and Language for
                        Precise Medical AI Assistant</span
                      >
                    </a>
                    <br />
                    Haonan Wang, Jiaji Mao, Lehan Wang, Qixiang Zhang, Marawan
                    Elbatel, <strong>Yi Qin</strong>, Huijun Hu, Baoxun Li,
                    Wenhui Deng, Weifeng Qin, Hongrui Li, Jialin Liang, Jun
                    Shen, <u>Xiaomeng Li</u>
                    <br />
                    <em>Arxiv</em>, 2025
                    <br />
                    <a href="https://arxiv.org/abs/2505.03380">Paper</a>
                    <p>
                      We introduce RCMed, a full-stack medical AI assistant that
                      enhances multimodal accuracy through hierarchical
                      vision-language grounding and a self-reinforcing
                      correlation loop. Trained on 20M samples, it excels in 165
                      clinical tasks across 9 modalities, achieving
                      state-of-the-art performance and strong generalization in
                      real-world cancer diagnosis and cell segmentation.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td style="padding: 20px; width: 25%; vertical-align: middle">
                    <img
                      src="./images/ECDM.png"
                      alt="b3do"
                      width="160"
                      style="border-style: none"
                    />
                  </td>
                  <td width="75%" valign="middle">
                    <a href="https://openreview.net/pdf?id=UKCPaTcJAo">
                      <span class="papertitle"
                        >Energy-Based Conceptual Diffusion Model</span
                      >
                    </a>
                    <br />
                    <strong>Yi Qin</strong>, Xinyue Xu, Hao Wang,
                    <u>Xiaomeng Li</u>
                    <br />
                    <em>Neurips Safe Generative AI Workshop</em>, 2024
                    <br />
                    <a href="https://openreview.net/pdf?id=UKCPaTcJAo">Paper</a>
                    <p>
                      We propose Energy-Based Conceptual Diffusion Models
                      (ECDMs), a framework that unifies the concept-based
                      generation, conditional interpretation, concept debugging,
                      intervention, and imputation under the joint energy-based
                      formulation.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td style="padding: 20px; width: 25%; vertical-align: middle">
                    <img
                      src="./images/CUDA.png"
                      alt="b3do"
                      width="160"
                      style="border-style: none"
                    />
                  </td>
                  <td width="75%" valign="middle">
                    <a href="https://arxiv.org/pdf/2505.05195">
                      <span class="papertitle"
                        >Concept-Based Unsupervised Domain Adaptation</span
                      >
                    </a>
                    <br />
                    Xinyue Xu, Yueying Hu, Hui Tang, <strong>Yi Qin</strong>, Lu
                    Mi, Hao Wang,
                    <u>Xiaomeng Li</u>
                    <br />
                    <em>ICML</em>, 2025
                    <br />
                    <a href="https://arxiv.org/pdf/2505.05195">Paper</a>
                    <p>
                      CUDA improves the robustness of Concept Bottleneck Models
                      under domain shifts by aligning concept representations
                      with adversarial training, allowing flexible differences,
                      and enabling concept inference without labels. It boosts
                      interpretability and outperforms state-of-the-art CBM and
                      domain adaptation methods.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <table
              width="100%"
              align="center"
              border="0"
              cellspacing="0"
              cellpadding="20"
            >
              <tbody>
                <tr>
                  <td>
                    <h2>Projects and Patents</h2>
                  </td>
                </tr>
              </tbody>
            </table>

            <table width="100%" align="center" border="0" cellpadding="20">
              <tbody>
                <tr>
                  <ul>
                    <li>
                      'The Blade Wall' - Interactive Computer Vision Art
                      Installation. Cooperated with DJI. Installed in DJI |
                      Hasselblad Mixed Flagship Store, Nanjing.
                    </li>
                    <li>
                      CN Patent [CN202211153269.4]
                      一种基于三维智能检测的智能调度方法 [实质审查]
                    </li>
                    <li>
                      CN Patent [CN202211156132.4]
                      一种基于体感的多电机阵列上位机控制系统 [实质审查]
                    </li>
                    <li>
                      CN Patent [CN202211153265.6]
                      一种基于体感的多电机阵列嵌入式底层驱动系统 [实质审查]
                    </li>
                    <li>
                      CN Patent [ZL202210346880.2]
                      一种基于Transformer的物流包裹分离方法 [公开]
                    </li>
                  </ul>
                </tr>
              </tbody>
            </table>

            <table
              width="100%"
              align="center"
              border="0"
              cellspacing="0"
              cellpadding="20"
            >
              <tbody>
                <tr>
                  <td>
                    <h2>Honors, Awards, and Services</h2>
                  </td>
                </tr>
              </tbody>
            </table>
            <table width="100%" align="center" border="0" cellpadding="20">
              <tbody>
                <ul>
                  <h3>Honors</h3>
                </ul>
                <tr>
                  <ul>
                    <li>ECE Best TA Award (2024/25) (5~7 Annually)</li>
                    <li>
                      HKUST RedBird Academic Excellence Award for Continuing PhD
                      Students (2024-25)
                    </li>
                    <li>HKUST RedBird PhD Recruitment Award (2023-24)</li>
                    <li>
                      Honoured Thesis - Diffusion Model-Empowered Unsupervised
                      Medical Image Registration
                    </li>
                    <li>
                      First Prize, School of Automation Science and Engineering
                      Scholarship (2021)
                    </li>
                  </ul>
                </tr>
              </tbody>
            </table>
            <table width="100%" align="center" border="0" cellpadding="20">
              <tbody>
                <ul>
                  <h3>Awards</h3>
                </ul>
                <tr>
                  <ul>
                    <li>
                      Metric Ranking #1/387, Overall Ranking #2,
                      <a
                        href="https://zindi.africa/competitions/kenya-clinical-reasoning-challenge"
                        >Kenya Clinical Reasoning Challenge</a
                      >, 2025
                    </li>
                    <li>Silver, Guangdong BME Innovative Competition, 2022</li>
                    <li>
                      Bronze & Best Strategy Award, China University Robot
                      Competition RoboMaster Competition, 2021
                    </li>
                    <li>
                      Third Prize, ICRA & RoboMaster 2021 AI Challenge, 2021
                    </li>
                  </ul>
                </tr>
              </tbody>
            </table>
            <table width="100%" align="center" border="0" cellpadding="20">
              <tbody>
                <ul>
                  <h3>Services</h3>
                </ul>
                <tr>
                  <ul>
                    <li>Reviewer for ICONIP2023, IEEE TNNLS, IEEE TPAMI</li>
                    <li>Challenge Organizer: <a href="https://github.com/xmed-lab/TriALS">TriALS@MICCAI 2025</a></li>
                    <li>
                      TA: ELEC 4840 Artificial Intelligence for Medical Image
                      Analysis (23/24 Spring, 24/25 Spring [Departmental Best
                      TA])
                    </li>
                    <li>
                      TA: ELEC 3300 Introduction to Embedded Systems (24/25
                      Fall)
                    </li>
                    <li>
                      Technical advisory board member: Guangdong QiLi Tech. Co.
                      Ltd.
                    </li>
                    <li>TED Talk Translator</li>
                  </ul>
                </tr>
              </tbody>
            </table>

            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td style="padding: 0px">
                    <br />
                    <p style="text-align: right; font-size: small">
                      This website is adapted from Jonathan Barron's
                      <a href="https://github.com/jonbarron/jonbarron_website"
                        >source code</a
                      >.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>
          </td>
        </tr>
      </tbody>
    </table>
  </body>
</html>
